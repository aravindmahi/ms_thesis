\chapter{Evaluations and Results}
\markright{Aravindan Mahendiran \hfill Chapter 4. Evaluations and Results \hfill}
To evaluate our hypothesis about the vocabularies we test our models on eight different presidential elections from Latin America using both the seed vocabulary and the vocabulary generated by the query expansion algorithm.
We use the results obtained using the seed vocabulary detailed in the previous section as a baseline score.
We then use the same vocabulary to seed our PSL learning algorithm. 
The prediction algorithms are then run again, now by using the expanded vocabulary obtained through the query expansion.
To maintain consistency, we track only the hashtags identified by 
the query expansion pipeline until that particular date (i.e., newly inferred hashtags are not used to
retroactively analyze older data).

\begin{figure*}
	\centering
	\includegraphics[scale=0.65]{support_files/Recall.png}
	\caption{Recall of seed vocabulary vs PSL vocabulary}
	\label{fig:recall}
\end{figure*}

Figure \ref{fig:recall} shows the increase in the number of documents that were used by the algorithms to make a predictions.
Note that when averaged across all the eight elections we obtain close to a two-fold increase in the number of 
tweets that were used by these models.
This is a substantial increase of relevant tweets for the domain.

	\begin{table*}
		\centering
		\scalebox{0.9}{
		\begin{tabular}{| l | l | r | r | r | r | r | r |}
		\hline
		Election & Candidate & Actual Result & Seed Vocab. & Error & PSL Vocab. & Error \\
		\hline
		\multirow{2}{*}{Mexico} & Pe\~{n}a Nieto & 38.1 & 49.26 & 11.11 & 46.43 & \textbf{8.28} \\\cline{2-7}
											   & L\'{o}pez Obrador & 31.64 & 25.11 & 6.53 & 27.65 & \textbf{4.00} \\
		\hline
		\multirow{2}{*}{Venezuela\_Oct7} & Hugo Ch\'{a}vez & 55.07 & 63.69 & 8.62 & 55.24 & \textbf{0.17}\\\cline{2-7}
																& Henrique Capriles & 44.31 & 36.31 & 8.00 & 44.76 & \textbf{0.45} \\
		\hline
		\multirow{2}{*}{Ecuador} & Rafael Correa & 57.16 & 32.36 & 24.81 & 32.90 & \textbf{24.27} \\\cline{2-7}
												 & Guillermo Lasso & 22.68 & 36.93 & \textbf{14.25} & 37.88 & 15.20 \\
		\hline
		 \multirow{2}{*}{Venezuela\_Apr15} & Nicol\'{a}s Maduro & 50.61 & 42.08 & 8.53 & 44.05 & \textbf{6.56} \\\cline{2-7}
																	& Henrique Capriles & 49.12 & 37.98 & \textbf{11.14} & 37.14 & 11.98 \\
		\hline
		\multirow{2}{*}{Paraguay} & Horacio Cartes & 48.48 & 29.80 & \textbf{18.68} & 29.12 & 19.36 \\\cline{2-7}
												   & Efra\'{i}n Alegre & 39.05 & 27.21 & \textbf{11.84} & 26.63 & 12.42 \\
		\hline
		\multirow{2}{*}{Chile\_Nov17} & Michelle Bachelet & 46.70 & 26.62 & 20.08 & 29.92 & \textbf{16.78}\\\cline{2-7}
														  & Evelyn Matthei & 25.03 & 18.76 & 6.27 & 19.52 & \textbf{5.51} \\
		\hline 
		\multirow{2}{*}{Honduras} & Orlando Hern\'{a}ndez & 36.80 & 28.94 & 7.86 & 34.74 & \textbf{2.06} \\\cline{2-7}
												   & Xiomara Castro & 28.70 & 9.67 & 19.03 & 14.20 & \textbf{14.50} \\
		\hline
		\multirow{2}{*}{Chile\_Dec15} & Michelle Bachelet & 62.16 & 57.66 & 4.50 & 59.24 & \textbf{2.92}\\\cline{2-7}
															& Evelyn Matthei & 37.83 & 42.34 & 4.51 & 40.67 & \textbf{2.84} \\
		\hline 											 
		\end{tabular}
		}
		\caption{Reduction in prediction error for Unique Visitor Model. All values shown are percentages.}
		\label{table:UniVis}
	\end{table*}

\begin{table*}
		\centering
		\scalebox{0.9}{
		\begin{tabular}{| l | l | r | r | r | r | r | r |}
		\hline
		Election & Candidate & Actual Result & Seed Vocab. & Error & PSL Vocab. & Error \\
		\hline
		\multirow{2}{*}{Mexico} & Pe\~{n}a Nieto & 38.1 & 46.80 & 8.65 & 39.00 & \textbf{0.85} \\\cline{2-7}
											   & L\'{o}pez Obrador & 31.64 & 24.67 & 6.97 & 28.64 & \textbf{3.00} \\
		\hline
		\multirow{2}{*}{Venezuela\_Oct7} & Hugo Ch\'{a}vez & 55.07 & 49.89 & 5.18 & 55.89 & \textbf{0.82}\\\cline{2-7}
																& Henrique Capriles & 44.31 & 36.31 & 8.00 & 43.91 & \textbf{0.40} \\
		\hline
		\multirow{2}{*}{Ecuador} & Rafael Correa & 57.16 & 53.33 & 3.84 & 54.33 & \textbf{2.84} \\\cline{2-7}
												 & Guillermo Lasso & 22.68 & 12.27 & 10.41 & 12.75 & \textbf{9.93} \\
		\hline
		 \multirow{2}{*}{Venezuela\_Apr15} & Nicol\'{a}s Maduro & 50.61 & 51.45 & 0.84 & 50.58 & \textbf{0.03} \\\cline{2-7}
																	& Henrique Capriles & 49.12 & 35.96 & 13.16 & 38.11 & \textbf{11.01} \\
		\hline
		\multirow{2}{*}{Paraguay} & Horacio Cartes & 48.48 & 35.21 & 13.27 & 40.63 & \textbf{7.85} \\\cline{2-7}
												   & Efra\'{i}n Alegre & 39.05 & 31.33 & 7.72 & 34.44 & \textbf{4.62} \\
		\hline
		\multirow{2}{*}{Chile\_Nov17} & Michelle Bachelet & 46.70 & 38.91 & 7.79 & 41.80 & \textbf{4.91}\\\cline{2-7}
														  & Evelyn Matthei & 25.03 & 19.20 & 5.83 & 20.98 & \textbf{4.05} \\
		\hline 
		\multirow{2}{*}{Honduras} & Orlando Hern\'{a}ndez & 36.80 & 25.16 & 11.64 & 28.30 & \textbf{8.50} \\\cline{2-7}
												   & Xiomara Castro & 28.70 & 16.53 & 12.17 & 24.90 & \textbf{3.80} \\
		\hline
		\multirow{2}{*}{Chile\_Dec15} & Michelle Bachelet & 62.16 & 39.12 & 23.04 & 39.80 & \textbf{22.37}\\\cline{2-7}
															& Evelyn Matthei & 37.83 & 20.88 & 16.95 & 21.68 & \textbf{16.15} \\
		\hline 											 
		\end{tabular}
		}
		\caption{Reduction in prediction error for Regression Model. All values shown are percentages.}
		\label{table:RegModel}
	\end{table*}	
	
To further illustrate the fact that the vocabulary used by such algorithms plays a vital role, we compare the performance of the models using the two different vocabularies.
To reduce the effect of outliers we track the popularity of only the top two candidates from each election.
Table~\ref{table:UniVis} shows the reduction in prediction error for the Unique Visitor model for each candidate if the expanded vocabulary from the PSL approach is used instead of the seed vocabulary.
On average the error was reduced by a 28.60\% from the original prediction error obtained by using seed vocabulary.
Similarly Table~\ref{table:RegModel} shows the reduction in error for the Regression Model.
Here an even better improvement of 45.19\% reduction in error was noted.
Averaging the reduction in error for both the models, the query expansion exercise was able to reduce the prediction error by  36.90\%.
We see greater and more consistent improvement with the regression model as the model weighs each window of tweets differently 
depending upon the opinion poll time series whereas the unique visitor model values them equally. 
Therefore, when the algorithm uses the `not-so-informative' hashtags identified during 
the earlier iterations, the sentiment value and the counts of these mentions bring down the accuracy of the model even though 
at a later stage hashtags that are more strongly indicative of a user's preference is picked up.
For instance, words such as ``facebook" which occur commonly dominate the counts and therefore skew the 
results even though they are dropped from the vocabulary at a later point.
\begin{figure*}
	\centering
	%\includegraphics[scale=0.65]{support_files/UVMevolution.png}
	\includegraphics[width=0.75\textwidth, height=0.3\textheight]{support_files/UVMevolution.png}
	\caption{Prediction Error for Unique Visitor Model over time}
	\label{fig:uvmEvolution}
\end{figure*}
\begin{figure*}
	\centering
	%\includegraphics[scale=0.65]{support_files/RMevolution.png}
	\includegraphics[width=0.75\textwidth, height=0.3\textheight]{support_files/RMevolution.png}
	\caption{Prediction Error for Regression Model over time}
	\label{fig:rmEvolution}
\end{figure*}
Figure~\ref{fig:uvmEvolution} compares the prediction accuracy of the two vocabularies for the Unique Visitor Model across the month leading up to the elections. 
The average error in prediction for the top two candidates is plotted on the vertical axis and the horizontal axis denotes the iteration number at the end of which the predictions were made.
Note that as the vocabulary grows and improves the prediction accuracy also correspondingly improves.

Similarly Figure ~\ref{fig:rmEvolution} compares the accuracy for Regression Model. 
This plot does not show the same trend as the previous plot for Unique Visitor model primarily because of the errors in the opinion poll data which are used as the dependent variable in the regression fit. 
However despite the noisy data it can be clearly seen that the vocabulary obtained through the query expansion exercise clearly out performs the static seed vocabulary.