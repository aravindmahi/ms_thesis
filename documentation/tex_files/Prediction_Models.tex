\chapter{Prediction Models}
\markright{Aravindan Mahendiran \hfill Chapter 3. Prediction Models \hfill}
In this section we review two prediction models we adapted from current literature to test our hypothesis. 
The first one is a naive model that forecasts elections based on the counts of mentions of a candidate.
We dub this as  \emph{"unique visitor model"} and is adapted from \cite{saez2011total} and \cite{tumasjan2010predicting}.
The second model uses a regression fit to regress from tweet features to opinion polls and then predicts election. 
This we dub as the \emph{"regression model"} and is adapted from \cite{bermingham2011using} and \cite{o2010tweets}.
\section{Unique Visitor Model}
Without any loss of generality, it can be assumed that large parties that are more popular will have a larger social media foot print than smaller and less popular parties. 
This model takes advantage of this assumption and predicts elections by calculating the relative popularity of candidates contesting the election.
We first define a vocabulary for each candidate. 
This vocabulary is crafted by hand and includes the candidate's names and aliases, the name and acronyms for his/her political party and the official Twitter handle of the candidate and is same as the seed vocabulary that was used to initialize the PSL pipeline.
For the given time period, the tweets from the country in question are tracked for the occurrence of the terms in the vocabulary.
We then build a time series of sentiment and Klout scores from the tweets returned.
Klout score is a value provided by Klout.com that quantifies the impact each user has on social media. 
We use the sentiment scores provided as a part of the meta-data of the tweet. 
%The sentiment scores typically fall in the range of $[-15,15]$ and is provided by Lexalytics - a pioneer in linguistic processing.
Once a time series of the Klout and sentiment scores are built, we calculate the absolute popularity of a candidate $C_d$ as:
\begin{equation}
{C_d} = \sum_i K_i * UCS_{id}
\end{equation}
where $K_i$ is the Klout score for user $i$, and $UCS_{id}$ is User Candidate Score, the average of sentiment scores for all tweets from user $i$ about candidate $d$.
We then normalize the popularity scores across all candidates so that they sum to $1$.
This gives us the relative popularity of each candidate $P_d$ using which we predict the elections.
\begin{equation}
{P_d} = \sum_i \frac{C_d}{C_i}
\end{equation}
From the above equations, it is noticeable that each user contributes only once to the popularity score of a candidate.
This was preferred to merely counting the mentions of a candidate since we wanted to remove the bias of bot generated tweets from election campaigns that boosted the number of times a candidate is mentioned on Twitter.
\section{Regression Model}
In this model, in addition to Twitter data, we leverage the opinion polls available for the elections to make our predictions.
Like the earlier model we track the tweets that contain a word from the vocabulary defined for each candidate.
We then define a linear regression fit that uses the opinion polls as dependent variable and features generated from these tweets as independent variable.
We use a total of 6 features based on Klout scores, number of unique users, total number of mentions, sentiment and incumbency.
We normalize each of these features across all candidates to get the relative share of the volume. 
For example for the we define share of positive mentions($SoPM$)  as: 
\begin{equation}
SoPM(x) = \frac{\#PositiveMentions(x)}{\sum_i \#PositiveMentions(i)} 
\end{equation}
and share of negative users($SoNU$) as:
\begin{equation}
SoNU(x) = \frac{\sum_j K_j}{\sum_i \sum_j K_j}
\end{equation}
where $K_j$ is the Klout score of user $j$ who tweeted negatively about a candidate.
Similarly we define share of sentiment ($SoS$) as the sum of all sentiment scores normalized across all candidates. 
We use a binary variable for incumbency. 
We then build a time-line of opinion polls. 
For each of the polling dates we calculate these features by using tweets created during the 10 day window leading up to the polling date.
When we have more than one polling house publishing its opinion poll for the same date we take the average of the polls. 
Once we create a feature set for all the polling dates, we fit a simple least square regression as :
\begin{equation}
\begin{split}
Popularity(x) = \alpha_1 * SoPM(x) + \alpha_2 * SoNM(x) \\
						 + \beta_1 * SoPU(x) + \beta_2 * SoNU(x) \\
						 + \gamma * SoS(x) + \delta * Incumbency(x) + \epsilon
\end{split}
\end{equation}
Table\ref{table:coeff} details the coefficients learned for each feature averaged over all the candidates from all the elections.
The values confirm our hypothesis that the number of unique users and sentiment have more predictive power than total number of mentions.
Intuitively it is also seen that the coefficients for share of negative users and negative mentions carry a negative weight.
Another interesting observation is the fact that the incumbency binary variable is not very predictive which is contradictory to the popular opinion.
\begin{table*}
        \centering
        \begin{tabular}{|l|r|}
        \hline
        Feature & Coefficient Value\\
        \hline
        $SoPU$ & 0.4622\\
        $SoNU$ & -0.443\\
        $SoPM$ & 0.1158\\
        $SoNM$ & -0.065\\
        $SoS$ & 0.156\\
        $Incumbency$ & 0.0\\
        \hline
        \end{tabular}
        \caption{Regression coefficients learned for features}
        \label{table:coeff}
\end{table*}
After learning the regression fit, we make a prediction by building such features using the same 10 day window leading up to the prediction date.
\section{Performance}
\begin{table*}
        \centering
        \begin{tabular}{| l | r | r | r |}
        \hline
        Election Type & Number of Elections & Number of Correct Predictions & Accuracy\\
        \hline
        President/Prime Minister & 8 & 8 & 100\%\\
        Governor & 4 & 3 & 75\%\\
        Mayor & 24 & 12 & 50\%\\
        Overall & 36 & 23 & 63.88\%\\
        \hline
        \end{tabular}
        \caption{Track Record of Prediction Algorithms}
        \label{table:trackRecord}
\end{table*}
The Unique Visitor Model and the Regression Model were tested exhaustively on a total of 36 elections from Latin America during 2012 and 2013 ranging from local mayor elections to presidential elections at the country level.
It is important to note that every single election was predicted ahead of time and not in retrospect.
The tweets were purchased from DataSift, an infoveilence service that resells Twitter data.
On an average we collected close to 2 million unique tweets a day from over 21 countries in Latin and South America.
Then these tweets were geo-coded using a geo-location algorithm we developed to obtain tweets from the country of interest.
Only tweets from the locations pertaining to elections were used to make the predictions.
For example, for the Rio De Janeiro Mayor elections only tweets from the city of Rio De Janeiro were used and similarly for state level Governor elections only tweet originating from that particular state was used.
Once the tweets were filtered by location the time series of Klout and sentiment scores were calculated by tracking the tweets for the mentions of candidate.\\
Table\ref{table:trackRecord} below shows the over all performance of the two models. 
It can be noticed that the accuracy drops as the granularity of the elections reduces. 
This is primarily due to the fact that, opinion polls were available only for the country level elections.
Therefore, we could not use the Regression Model for the state or city level elections.
This increased the error as the predictions were generated only from the naive Unique Visitor Model.\\
Also, from the tweets collected it was noticed that there wasn't much chatter on Twitter about these smaller local elections.
This skewed the results as the model tracking the names of the candidates was using tweets that mentioned the candidate's name but wasn't about the candidate contesting in the election but was about some other person having the same name as the candidate.
If the city level elections were ignored as outliers the over all accuracy of the models improves to 91.6\%.
